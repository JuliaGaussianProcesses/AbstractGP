<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Parametric Heteroscedastic Model · AbstractGPs.jl</title><meta name="title" content="Parametric Heteroscedastic Model · AbstractGPs.jl"/><meta property="og:title" content="Parametric Heteroscedastic Model · AbstractGPs.jl"/><meta property="twitter:title" content="Parametric Heteroscedastic Model · AbstractGPs.jl"/><meta name="description" content="Documentation for AbstractGPs.jl."/><meta property="og:description" content="Documentation for AbstractGPs.jl."/><meta property="twitter:description" content="Documentation for AbstractGPs.jl."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">AbstractGPs.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../api/">The Main APIs</a></li><li><a class="tocitem" href="../../concrete_features/">Concrete Features</a></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../0-intro-1d/">Intro to AbstractGPs: one-dimensional regression</a></li><li><a class="tocitem" href="../1-mauna-loa/">Mauna Loa time series example</a></li><li><a class="tocitem" href="../2-deep-kernel-learning/">Deep Kernel Learning with Flux</a></li><li class="is-active"><a class="tocitem" href>Parametric Heteroscedastic Model</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Examples</a></li><li class="is-active"><a href>Parametric Heteroscedastic Model</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Parametric Heteroscedastic Model</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaGaussianProcesses/AbstractGPs.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaGaussianProcesses/AbstractGPs.jl/blob/master/examples/3-parametric-heteroscedastic/script.jl" title="View source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Parametric-Heteroscedastic-Model"><a class="docs-heading-anchor" href="#Parametric-Heteroscedastic-Model">Parametric Heteroscedastic Model</a><a id="Parametric-Heteroscedastic-Model-1"></a><a class="docs-heading-anchor-permalink" href="#Parametric-Heteroscedastic-Model" title="Permalink"></a></h1><p><a href="https://nbviewer.jupyter.org/github/JuliaGaussianProcesses/AbstractGPs.jl/blob/gh-pages/dev/examples/3-parametric-heteroscedastic/notebook.ipynb"><img src="https://img.shields.io/badge/show-nbviewer-579ACA.svg" alt/></a></p><p><em>You are seeing the HTML output generated by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a> from the <a href="https://github.com/JuliaGaussianProcesses/AbstractGPs.jl/blob/master/examples/3-parametric-heteroscedastic/script.jl">Julia source file</a>. The corresponding notebook can be viewed in <a href="https://nbviewer.jupyter.org/github/JuliaGaussianProcesses/AbstractGPs.jl/blob/gh-pages/dev/examples/3-parametric-heteroscedastic/notebook.ipynb">nbviewer</a>.</em></p><hr/><p>This example is a small extension of the standard GP regression problem, in which the observation noise variance is a function of the input. It is assumed to be a simple quadratic form, with a single unknown scaling parameter, in addition to the usual lengthscale and variance parameters of the GP. A point estimate of all parameters is obtained using type-II maximum likelihood, as per usual.</p><pre><code class="language-julia hljs">using AbstractGPs
using AbstractGPsMakie
using CairoMakie
using KernelFunctions
using Optim
using ParameterHandling
using Zygote

using LinearAlgebra
using Random
Random.seed!(42)  # setting the seed for reproducibility of this notebook</code></pre><p>In this example we work with a simple GP with a Gaussian kernel and heteroscedastic observation variance.</p><pre><code class="language-julia hljs">observation_variance(θ, x::AbstractVector{&lt;:Real}) = Diagonal(0.01 .+ θ.σ² .* x .^ 2)
function build_gpx(θ, x::AbstractVector{&lt;:Real})
    Σ = observation_variance(θ, x)
    return GP(0, θ.s * with_lengthscale(SEKernel(), θ.l))(x, Σ)
end;</code></pre><p>We specify the following hyperparameters:</p><pre><code class="language-julia hljs">const flat_θ, unflatten = ParameterHandling.value_flatten((
    s=positive(1.0), l=positive(3.0), σ²=positive(0.1)
));
θ = unflatten(flat_θ);</code></pre><p>We generate some observations:</p><pre><code class="language-julia hljs">const x = 0.0:0.1:10.0
const y = rand(build_gpx(θ, x));</code></pre><p>We specify the objective function:</p><pre><code class="language-julia hljs">function objective(flat_θ)
    θ = unflatten(flat_θ)
    fx = build_gpx(θ, x)
    return -logpdf(fx, y)
end;</code></pre><p>We use L-BFGS for optimising the objective function. It is a first-order method and hence requires computing the gradient of the objective function. We do not derive and implement the gradient function manually here but instead use reverse-mode automatic differentiation with Zygote. When computing gradients with Zygote, the objective function is evaluated as well. We can exploit this and <a href="https://julianlsolvers.github.io/Optim.jl/stable/#user/tipsandtricks/#avoid-repeating-computations">avoid re-evaluating the objective function</a> in such cases.</p><pre><code class="language-julia hljs">function objective_and_gradient(F, G, flat_θ)
    if G !== nothing
        val_grad = Zygote.withgradient(objective, flat_θ)
        copyto!(G, only(val_grad.grad))
        if F !== nothing
            return val_grad.val
        end
    end
    if F !== nothing
        return objective(flat_θ)
    end
    return nothing
end;</code></pre><p>We optimise the hyperparameters using initializations close to the values that the observations were generated with.</p><pre><code class="language-julia hljs">flat_θ_init = flat_θ + 0.01 * randn(length(flat_θ))
result = optimize(
    Optim.only_fg!(objective_and_gradient),
    flat_θ_init,
    LBFGS(;
        alphaguess=Optim.LineSearches.InitialStatic(; scaled=true),
        linesearch=Optim.LineSearches.BackTracking(),
    ),
    Optim.Options(; show_every=100),
)</code></pre><pre><code class="nohighlight hljs"> * Status: success

 * Candidate solution
    Final objective value:     1.576205e+02

 * Found with
    Algorithm:     L-BFGS

 * Convergence measures
    |x - x&#39;|               = 3.97e-08 ≰ 0.0e+00
    |x - x&#39;|/|x&#39;|          = 1.58e-08 ≰ 0.0e+00
    |f(x) - f(x&#39;)|         = 0.00e+00 ≤ 0.0e+00
    |f(x) - f(x&#39;)|/|f(x&#39;)| = 0.00e+00 ≤ 0.0e+00
    |g(x)|                 = 6.34e-09 ≤ 1.0e-08

 * Work counters
    Seconds run:   1  (vs limit Inf)
    Iterations:    10
    f(x) calls:    13
    ∇f(x) calls:   11
</code></pre><p>The optimal model parameters are:</p><pre><code class="language-julia hljs">θ_final = unflatten(result.minimizer)</code></pre><pre><code class="nohighlight hljs">(s = 0.385143986830958, l = 2.3342748073768, σ² = 0.08082582089918736)</code></pre><p>We compute the posterior GP with these optimal model parameters:</p><pre><code class="language-julia hljs">fx_final = build_gpx(θ_final, x)
f_post = posterior(fx_final, y);</code></pre><p>We visualize the results with <a href="https://github.com/JuliaGaussianProcesses/AbstractGPsMakie.jl">AbstractGPsMakie</a>:</p><pre><code class="language-julia hljs">using CairoMakie.Makie.ColorSchemes: Set1_4

with_theme(
    Theme(;
        palette=(color=Set1_4,),
        patchcolor=(Set1_4[2], 0.2),
        Axis=(limits=((0, 10), nothing),),
    ),
) do
    plot(
        x,
        f_post(x, observation_variance(θ_final, x));
        bandscale=3,
        label=&quot;posterior + noise&quot;,
        color=(:orange, 0.3),
    )
    plot!(x, f_post; bandscale=3, label=&quot;posterior&quot;)
    gpsample!(x, f_post; samples=10, color=Set1_4[3])
    scatter!(x, y; label=&quot;y&quot;)
    axislegend()
    current_figure()
end</code></pre><p><img src="index-22.png" alt/></p><hr />
<h6>Package and system information</h6>
<details>
<summary>Package information (click to expand)</summary>
<pre>
Status &#96;~/work/AbstractGPs.jl/AbstractGPs.jl/examples/3-parametric-heteroscedastic/Project.toml&#96;
  &#91;99985d1d&#93; AbstractGPs v0.5.21 &#96;/home/runner/work/AbstractGPs.jl/AbstractGPs.jl#master&#96;
  &#91;7834405d&#93; AbstractGPsMakie v0.2.6
⌃ &#91;13f3f980&#93; CairoMakie v0.10.12
  &#91;ec8451be&#93; KernelFunctions v0.10.63
  &#91;98b081ad&#93; Literate v2.16.1
  &#91;429524aa&#93; Optim v1.9.2
  &#91;2412ca09&#93; ParameterHandling v0.5.0
  &#91;e88e6eb3&#93; Zygote v0.6.69
  &#91;37e2e46d&#93; LinearAlgebra
  &#91;9a3f8284&#93; Random
Info Packages marked with ⌃ have new versions available and may be upgradable.
</pre>
To reproduce this notebook's package environment, you can
<a href="./Manifest.toml">
download the full Manifest.toml</a>.
</details>
<details>
<summary>System information (click to expand)</summary>
<pre>
Julia Version 1.10.2
Commit bd47eca2c8a &#40;2024-03-01 10:14 UTC&#41;
Build Info:
  Official https://julialang.org/ release
Platform Info:
  OS: Linux &#40;x86_64-linux-gnu&#41;
  CPU: 4 × AMD EPYC 7763 64-Core Processor
  WORD_SIZE: 64
  LIBM: libopenlibm
  LLVM: libLLVM-15.0.7 &#40;ORCJIT, znver3&#41;
Threads: 1 default, 0 interactive, 1 GC &#40;on 4 virtual cores&#41;
Environment:
  JULIA_DEBUG &#61; Documenter
  JULIA_LOAD_PATH &#61; :/home/runner/.julia/packages/JuliaGPsDocs/7M86H/src
</pre>
</details><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../2-deep-kernel-learning/">« Deep Kernel Learning with Flux</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.3.0 on <span class="colophon-date" title="Monday 11 March 2024 12:34">Monday 11 March 2024</span>. Using Julia version 1.10.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
